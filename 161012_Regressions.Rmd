---
title: "Regressions"
author: "Juan Rocha"
date: "12 October 2016"
output: 
  html_document:
    self_contained: true
    toc: true
    deep: 2
    toc_float:
      collapsed: true
      smooth_scroll: true
    fig_width: 5
    fig_height: 5
    dev: png
    code_folding: hide  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library (plm)
library(ggplot2)
library(tidyr)
library(dplyr)
library(RColorBrewer)
library(GGally)
# library(vegan)
# library(cluster)
# library(NbClust);library(kohonen)
# library(mclust); library(clValid)
library(ineq) # for gini
```

```{r setup_data, include = FALSE}


# setwd("~/Dropbox/BEST/Colombia/0_Game data") # here is the data
dat <- read.csv(file="~/Dropbox/BEST/Colombia/0_Game data/160427_corrected_full_data_long.csv", row.names=1) # in long format, short format also available

# set directory for figures
# setwd('~/Documents/Projects/BEST - Beijer/BEST')

## Preliminary data exploration

names(dat)
str(dat)
summary(dat)

# Correct & unify place, treatment names
# levels(dat$Place)[3] <- "Las Flores"
# levels(dat$Treatment)[c(1,3)] <- 'Uncertainty'
# levels(dat$Treatment)[c(2,7)] <- 'Risk'
# levels(dat$Treatment)[c(3,5)] <- 'Base line'
# levels(dat$Treatment)[c(4,5)] <- 'Threshold'

# Correct date, session, player as factor
# dat$Date <- as.factor(dat$Date)
# levels(dat$Date) <- c('2016-02-09', '2016-02-01', '2016-02-02','2016-02-03','2016-02-04','2016-02-05','2016-02-10','2016-02-12') # standard dates
# levels(dat$Session) <- c('am','pm')
# levels(dat$Player) <- c('1','2','3','4')

# dat$part <- dat$Round > 6

# Create player ID's as in Surveys.R
dat <- transform (dat, ID_player = interaction(Date, Treatment, Session, Player, drop = TRUE))
# Create ID group
dat <- transform(dat, group = interaction (Date, Treatment, Session, drop=T))

# We need to make NA explicit: this is, rounds that were not played (as zeroes) because resource was collapsed
summary (dat)
dat2 <- dplyr::select(dat, -StockSizeBegining, -SumTotalCatch, -IntermediateStockSize, -Regeneration, -NewStockSize,-part) %>%
  spread(key=Round, value=value)

dat3 <- dplyr::select(dat2, 8:23)
dat3 <- as.matrix(dat3)
dat3[is.na(dat3)] <- 0
dat2[,8:23] <- dat3

dat3 <- dat2 %>%
  gather(Round, value, 8:23)
dat3$Round <- as.numeric(dat3$Round)

dat <- full_join(dat3, dat)
str(dat)
summary(dat)
dat <- gdata::drop.levels(dat)

dat.noNA <- dat
summary (dat.noNA)
dat.noNA$StockSizeBegining[is.na(dat.noNA$StockSizeBegining)] <-  0
dat.noNA$SumTotalCatch[is.na(dat.noNA$SumTotalCatch)] <-  0
dat.noNA$IntermediateStockSize[is.na(dat.noNA$IntermediateStockSize)] <-  0
dat.noNA$Regeneration[is.na(dat.noNA$Regeneration)] <-  0
dat.noNA$NewStockSize[is.na(dat.noNA$NewStockSize)] <-  0

dat <- dat.noNA
# write.csv(dat, file='160427_corrected_full_data_long.csv')

# check that ID's are equal in both datasets
# levels(dat$ID_player) %in% levels(surv.dat$ID_player)
# levels(surv.dat$ID_player) %in% levels(dat$ID_player)
# 
# # Now you can join them and use both datasets for stats!!
# full <- full_join(dat, surv.dat, by= c('ID_player' = 'ID_player', 'Session' = 'Session', 
#                                         'Date' = 'date', 'Place' = 'locationName', 'Round'='round'))
# str(full)


dat <- mutate (dat, crossThreshold = ifelse(dat$Treatment == 'Base line' | dat$part == FALSE, 
                                            dat$IntermediateStockSize - 20, 
                                            dat$IntermediateStockSize - 28))

dat <- mutate (dat, threshold = ifelse (dat$Treatment == "Base line" | dat$part == FALSE, 20, 28 ))

# dat <- mutate (dat, crossThreshold = StockSizeBegining - 28)

# 
# str(dat)
# summary(dat)

## Use the deviation from threshold, and dev_t_divided by 4

dat <- dat %>%
  mutate (dev_drop = ifelse(dat$Treatment == 'Base line' | dat$part == FALSE,
                                ((dat$IntermediateStockSize - 20)) ,  # - dat$value
                                 ((dat$IntermediateStockSize - 28))   )) #- dat$value

dat <- dat %>%
  mutate (optimal = (StockSizeBegining - threshold) / 4) %>%
  mutate (cooperation = optimal - value)

# create dummies for Treatments 0 for rounds 1-6, 1 for 7-16, and 1 for all base line.
dat <- dat %>% 
  mutate (BL = ifelse (dat$Treatment == "Base line" | dat$part == FALSE , 1,0) ,
          TR = ifelse (dat$Treatment == 'Threshold' & dat$part == TRUE, 1, 0) , 
          U = ifelse (dat$Treatment == 'Uncertainty' & dat$part == TRUE, 1, 0) , 
          R = ifelse (dat$Treatment == 'Risk' & dat$part == TRUE, 1, 0) 
          )

### Group level data
group_dat <- dat %>%
  select (Treatment, Place, group, Round, StockSizeBegining, IntermediateStockSize, 
          Regeneration, NewStockSize, part, BL,TR,U,R) %>%
  unique ()

```

# Analysis at the individual level
Note to self: Don't look at the text, just check results. I'm fitting models only taking into account the second part of the game.

### Definition of cooperation: an individual based analysis

Cooperation is measured as _the right thing to do_ minus what people actually did, aka. _value_. The _right thing to do_ can be approximated as the _StockSizeBegining_ minus _Threshold_. The later is the drop point on the reproduction rate of the stock, which is 20 for Base line and 28 for other treatments. Therefore, if cooperation is zero, is at its maximum value, if it's >0 it means people did cooperate in order to avoid the threshold but were not efficient at maximizing their personal utility; if <0 it means people did not cooperate and prefered maximizing their utility over the common good of maintaining the resource on the long run. This is how cooperation look for our `r dim(dat)[1]` observations:

***

```{r coop, echo = FALSE, fig.height= 4, fig.width= 4, fig.align='center'}
plot (density(dat$cooperation), main= "Cooperation at individual level")
```

### Ordinary least squares approximation {.smaller}
It's a naive approach that doesn't take into consideration heterogeneity across groups or time.
```{r ols}
ols <- lm(cooperation ~ Treatment + Place, data = filter (dat, part == TRUE))
summary (ols)
```

*** 

```{r mod1, fig.height= 3, fig.width=3}
plot(ols, cex = 0.8)
```

### Anova:  Analisis of variance
The analysis of variance reveals that there is significant differences between Treatments and Place. The Tukey test shows the pair wise comparisons between treatments and places. At the treatment level, uncertainty and base line are not significantly different, neither threshold from risk; all other pair wise combinations are significant. For location effects, only Tasajeras and Tanga are not significantly different, all other pairwise combinatios are significant. Similar results were presented at CCS with the Pairwise Wilcox test not shown here.
```{r anova}
anova (ols)

aov.mod <- aov (cooperation ~ Treatment + Place, data = filter (dat, part == TRUE))

TukeyHSD(aov.mod)
```

### Fixed effects using least squares dummy variable model {.smaller}
```{r fix_effects}
fixed.dum <- lm(cooperation ~ Treatment + factor (Place) - 1, data = filter (dat, part == TRUE) )
summary(fixed.dum)
```

### Panel data model with fixed effects {.smaller}
```{r plm_fixed}
fixed <- plm (cooperation ~  Treatment , data = filter (dat, part == TRUE), index = c('ID_player' ,'Round'), model = 'within')
summary (fixed)
```

### Panel data model with random effects {.smaller}
```{r random}
random <- plm (cooperation ~ U + R + TR , data = filter (dat, part == TRUE) , index = c('ID_player' ,'Round'), model = 'random')
summary (random)
```

### fixed or random
p-value is < 0.05 then we should use **fixed** effects model at the individual level. If I remember correctly in our conversations with Matias, this is good news in the sense that we can introduce the fixed effects directly on the regression, where the fixed effects (alpha parameter) will use e.g. the info from the surveys directly as regression terms. This interpretation needs to be checked with Caro, Mattias or an statistician. 

```{r test1}
phtest(fixed, random)
```


### Include time fixed effects {.smaller}
Note that in most of the models the signficance of treatment change if I use the time effects as categorical varialbe or numeric. If numeric, the model uses only one parameter to estimate the effect of time (Round); but if categorical, there is one parameter for each time step fitted to the model. What is the best way to fit it? 

From Caroline's results in STATA, it seems that only one parameter is the default, while for R it automatically convert the time parameter into a `factor`. I can reverse the behaviour by using `as.numeric` but needs to be consulted with statistician. Results below shows the longer model. Note that in the STATA tutorial from Princeton, time is treated as factor as well. I also compare between fixed and random time effects with the Hausman test which suggest random.

```{r time, echo = FALSE}
random.time <- plm (cooperation ~   U + R + TR +  Round, data = filter (dat, part == TRUE) , index = c('ID_player' ,'Round'), model = 'random')
summary (random.time)
fixed.time <- plm (cooperation ~ U + R + TR +  Round , data = filter (dat, part == TRUE) , index = c('ID_player' ,'Round'), model = 'within')
summary (fixed.time)
phtest(fixed.time, random.time) # use random
```


### Testing for random effects {.smaller}
Below I test a OLS-pooled model. Intriguintly, when modeling time as numeric or as factor not only change the sign of the coefficient but also who is significant. Using time as factor (default in R) renders all signficant except Uncertainty, which has a negative coefficient and p-value > 0.1. Using time as numeric, results on positive and significant coefficient for Uncertainty and negative and non-significant coefficient for Risk. To follow the tutorial below the model is fitted without the time effects.

```{r pool}
pool <- plm(cooperation ~  U + R + TR  , #+ as.numeric(Round)
            data = filter (dat, part == TRUE) , index = c('ID_player' ,'Round'), 
            model = 'pooling')

summary (pool)
```

### Breusch-Pagan Lagrange multiplier (LM)
The Lagrange multiplier helps decide between random effects regression or a simple OLS regression. The null hypothesis is that variances across entitites is zero, thus no significant difference across units (no panel effect). If p<0.05 a random model is preferred, meaning there is evidence that there is differences amongst treatments. 

```{r test2}
plmtest(pool, type = 'bp')

```

### Testing for cross-sectional dependnece
The cross-sectional dependence test check if the residuals are correlated accross entities (here groups). It seems that there is cross-sectional dependence, probably the location effect. Cross-sectional dependence can lead to bias in test results (contemporaneous correlation). In the tutorial this test is applied to the fixed effects model, which I do below. However, the Haussman test above suggest random effects model is more suitable for the data. Does it make sense to make the cross-sectional dependence on random effects model? If so, both test are positive suggesting cross-sectional dependence on random model. Current test on fixed model suggest cross-sectional dependence (results below), and the same was found for random model and random model with fixed time effects). 
```{r test3}
pcdtest(fixed, test = c('lm'))
pcdtest(fixed, test = c("cd"))

# pcdtest(random, test = c('lm'))
# pcdtest(random, test = c("cd"))
# 
# pcdtest(random.time, test = c('lm'))
# pcdtest(random.time, test = c("cd"))
```


### Testing for serial correlation
There is serieal correlation (p < 0.05). The null hypothesis is that residuals across entities are not correlated, here entities I believe is groups. It shouldn't be a problem in micropanels with few time steps like ours, but we have it both on the random model and the random with time effects. Not sure this change if time is assumed as factor.
```{r test4}
pbgtest(random)
```

### Dickey-Fuller test
It check for stochastic trends, the null hypothesis is that the series as a unit root (non-stationary). If root is present one can take the first difference of the variable. p-value is < 0.05 then there is no unit roots present.
```{r test5, warning = FALSE}
panel.set <- plm.data (filter (dat, part == TRUE) , index = c('ID_player' ,'Round') ) 
library(tseries)
adf <-  adf.test (panel.set$cooperation, k=2)
adf
```

### Breusch-Pagan test for homoskedasticity
p < 0.05 therefore there is presence of heteroskedasticity, one can use robust covariance matrix to account for it. Both tutorials in R and STATA suggest good ways of dealing with heteroskedasticity but I will stop here. First we need to define which model are we fitting and presenting on the papers: fixed, random, with treatment + location + time effects?

```{r test6, echo = FALSE, warning= FALSE}
library(lmtest)
bptest(cooperation ~  U + R + TR , data = filter (dat, part == TRUE) , studentize = F )
```

# Analysis at the group level
At the group level the response variable is _IntermediateStockSize_, else remains the same. The dataset used is therefore smaller: `r dim(group_dat)[1]` observations. Note that when I clean the data I set up all rounds not played to zeroes. It means that rounds not played due to errors (experimenter forgot to play one round on one group) does not show on my dataset and i cannot exclude it at the moment. However, below I show there is not difference from Caro's analysis where she does exclude it. I will follow the same steps as for the individual analysis.

### Ordinary least squares approximation {.smaller}
OLS is a naive approach that doesn't take into consideration heterogeneity across groups or time, but needs to be tested for comparison purposes to see which model is better suited to our data.
```{r g_ols}
ols <- lm(IntermediateStockSize ~ Treatment + Place, data = filter (group_dat, part == TRUE))
summary (ols)
```

*** 

```{r g_mod1, fig.height= 3, fig.width=3}
plot(ols, cex = 0.8)
```

### Anova:  Analisis of variance
Similar to the Anova for the individual level models, here it shows that there is significant differences for place and treatment. The Tukey test shows that the differences are significant for all pairwise comparison except for Threshold and Risk treatments, and Tasajera-Taganga locations. These results agree with Wilcox pair wise comparisons presented at CCS16.
```{r g_anova}
anova (ols)
aov.mod2 <- aov (IntermediateStockSize ~ Treatment + Place, data = filter (group_dat, part == TRUE))
TukeyHSD(aov.mod2)
```

### Fixed effects using least squares dummy variable model {.smaller}
I don't understand why the basic linear model in R does drop one location but doesn't do the same with Treatment. Both variables are coded as factors.
```{r g_fix_effects}
fixed.dum <- lm(IntermediateStockSize ~ Treatment - 1 + factor (Place) - 1, data = filter (group_dat, part == TRUE) )
summary(fixed.dum)
```

### Panel data model with fixed effects {.smaller}
Here I drop the location effect and try to follow exactly the same model Caro fitted both for fixed effects and random effects below, but without time effects. Both models show very low significance for selected treatments: risk in random model, and risk & threshold for fixed model. 
```{r g_plm_fixed_random}
fixed <- plm (IntermediateStockSize ~  U + R + TR , data = filter (group_dat, part == TRUE), index = c('group' ,'Round'), model = 'within')
summary (fixed)
random <- plm (IntermediateStockSize ~   U + R + TR , data = filter (group_dat, part == TRUE) , index = c('group' ,'Round'), model = 'random')
summary (random)
phtest(fixed, random)
```

Hausman test p-value is > 0.05 then we should use random effect model. Not sure it holds for model with time treated as factor.

## Include time fixed effects {.smaller}
Now I'm fitting the same model Caroline did on the presentation. Note that now the random effects model show the same p-values and coefficients as Caroline's model in STATA. So dropping the missing round does not make a difference on model output, she uses 1023 obs while I do 1024. However, bear in mind that if I fit the model with time as factor (default for R and as it's shown in tutorial), the significant effect treatment Threshold dissapears.
```{r g_time, echo = FALSE}
random.time <- plm (IntermediateStockSize ~  U + R + TR + as.numeric(Round) , data = filter (group_dat, part == TRUE) , index = c('group' ,'Round'), model = 'random')
summary (random.time)
fixed.time <- plm (IntermediateStockSize ~   U + R + TR + as.numeric(Round) , data = filter (group_dat, part == TRUE) , index = c('group' ,'Round'), model = 'within')
summary (fixed.time)
phtest(fixed.time, random.time) # use random
```
Note that here Hausman test p > 0.05 suggesting fixed effects model. I'm not sure it holds when treating time as factor.

## Testing for random effects {.smaller}
```{r g_pool}
pool <- plm(IntermediateStockSize ~   U + R + TR + as.numeric(Round) ,
            data = filter (group_dat, part == TRUE) , index = c('group' ,'Round'), 
            model = 'pooling')

summary (pool)
```

### Breusch-Pagan Lagrange multiplier (LM)
The Lagrange multiplier helps decide between random effects regression or a simple OLS regression. If p<0.05 a random model is preferred, meaning there is evidence that there is differences amongst treatments.

```{r g_test2, warning= FALSE}
plmtest(pool, type = 'bp')
```

### Testing for cross-sectional dependnece
Both Breusch-Pagan LM test and Pesaran CD test indicate that there is cross-sectional dependence, probably the location effect. For the first paper with analysis on group level (Schill et al) Caro wanted to emphasize the results on the treatment effects rather than treatment and location. However, if all these test are common practice in econometrics, wouldn't be easier to present the complete history and the control for location already on the model, or perhaps show comparative models table?
```{r g_test3}
pcdtest(random, test = c('lm'))
pcdtest(random, test = c("cd"))
```

### Testing for serial correlation
There is serieal correlation (p-value < 0.05). It shouldn't be a problem in micropanels with few time steps like ours, but we have it. I believe it means that coefficients across time are correlated, but there is already a term for time effects on the model; so I'm not sure how to interpret this result. Maybe solved if time is treated as factor?
```{r g_test4}
pbgtest(random)
```

### Dickey-Fuller test
It check for stochastic trends, the null hypothesis is that the series as a unit root (non-stationary). If root is present one can take the first difference of the variable. p-value is < 0.05 then there is no unit roots present. Same results obtained at the individual level.
```{r g_test5, warnings = FALSE}
panel.set <- plm.data (filter (group_dat, part == TRUE), index = c('group' ,'Round')) 
library(tseries)
adf <-  adf.test (panel.set$IntermediateStockSize, k=2)
adf
```

### Breusch-Pagan test for homoskedasticity
p < 0.05 therefore there is presence of heteroskedasticity, one can use robust covariance matrix to account for it. First let's decide which models are we using and then decide how to fix this.

```{r g_test6}
library(lmtest)
bptest(IntermediateStockSize ~  BL + U + R + TR , data = filter (group_dat, part == TRUE) , studentize = F )
```

